// src/components/HelpModal.tsx
import React from 'react';
import Modal from './Modal';
import type { HelpModalProps } from '../../types';

const HelpModal: React.FC<HelpModalProps> = ({ isOpen, onClose }) => {
  return (
    <Modal isOpen={isOpen} onClose={onClose} title="Help & Instructions" widthClass="sm:max-w-2xl">
      <div className="space-y-4 text-slate-300 max-h-[70vh] overflow-y-auto custom-scroll pr-2">
        <section>
          <h4 className="text-lg font-semibold text-sky-400 mb-2">Getting Started</h4>
          <p>This application allows you to create interactive workflows for Large Language Models (LLMs) using Google's Gemini API.</p>
          <ol className="list-decimal list-inside space-y-1 mt-1">
            <li>Create a new project or open an existing one.</li>
            <li>Add nodes (Start, Prompt, Conditional, Variable, Conclusion) to the canvas.</li>
            <li>Name your nodes. For Prompt nodes, write LLM prompts. For Variable nodes, give them a concise name (e.g., 'customerQuery'). For Conclusion nodes, set a display title.</li>
            <li>Link nodes together to define the workflow data flow. Use the "Next Node" dropdown in the node editor.</li>
            <li>Use Conditional nodes to create branches based on LLM output.</li>
            <li>Use Variable nodes to store outputs for use in later prompts.</li>
            <li>Use Conclusion nodes to display the final output of a workflow path.</li>
            <li>Configure your Gemini API Key in "App Settings".</li>
            <li>Run your project and review the results!</li>
          </ol>
        </section>
        <hr className="border-slate-700"/>
        <section>
          <h4 className="text-lg font-semibold text-sky-400 mb-2">Node Types</h4>
          <ul className="space-y-1">
            <li><strong>Start Node:</strong> The entry point of your workflow. It has one output. Its prompt is sent to the LLM.</li>
            <li><strong>Prompt Node:</strong> Sends a prompt to the LLM. Can take input from a previous node using <code className="bg-slate-700 px-1 rounded text-sky-300">{'{PREVIOUS_OUTPUT}'}</code> and from Variable nodes using <code className="bg-slate-700 px-1 rounded text-sky-300">{'{variableName}'}</code>. Has one output.</li>
            <li><strong>Conditional Node:</strong> Evaluates the output from the previous node against defined conditions to decide the next step. Its prompt is sent to the LLM (can use variables).</li>
            <li><strong>Variable Node:</strong> Captures the output from the previous node and stores it under its name. This name (e.g., 'myVar') can then be used as a placeholder <code className="bg-slate-700 px-1 rounded text-sky-300">{'{myVar}'}</code> in subsequent Start, Prompt, or Conditional nodes. It does not execute an LLM prompt itself. Has one output.</li>
            <li><strong>Conclusion Node:</strong> Displays the output received from the previous node. It does not execute an LLM prompt and cannot be linked to a next node. Its "prompt" field in the editor is used as a display title.</li>
          </ul>
        </section>
        <hr className="border-slate-700"/>
        <section>
          <h4 className="text-lg font-semibold text-sky-400 mb-2">Using Placeholders in Prompts</h4>
          <p>In Start, Prompt, and Conditional Node prompts, you can use placeholders:</p>
          <ul className="list-disc list-inside space-y-1 mt-1">
            <li><code className="bg-slate-700 px-1 rounded text-sky-300">{'{PREVIOUS_OUTPUT}'}</code>: This is replaced with the text generated by the node that directly feeds into the current node.</li>
            <li><code className="bg-slate-700 px-1 rounded text-sky-300">{'{variableName}'}</code>: If you have a Variable Node named (for example) 'customerInfo', you can use <code className="bg-slate-700 px-1 rounded text-sky-300">{'{customerInfo}'}</code> in a later node's prompt. This will be replaced by the value that the 'customerInfo' Variable Node captured. Variable names should not contain spaces or special characters.</li>
          </ul>
          <p className="mt-1"><strong>Replacement Order:</strong> Custom variables like <code className="bg-slate-700 px-1 rounded text-sky-300">{'{variableName}'}</code> are replaced first, then <code className="bg-slate-700 px-1 rounded text-sky-300">{'{PREVIOUS_OUTPUT}'}</code> is replaced.</p>
          <p className="mt-1">Example: Node A (Start) outputs "The topic is AI ethics." This feeds into Node B (Variable) named 'mainTopic'. Node B feeds into Node C (Prompt).<br/>
          Node C's prompt: "Discuss <code className="bg-slate-700 px-1 rounded text-sky-300">{'{mainTopic}'}</code>. The previous direct input was: <code className="bg-slate-700 px-1 rounded text-sky-300">{'{PREVIOUS_OUTPUT}'}</code>."<br/>
          When Node C runs, <code className="bg-slate-700 px-1 rounded text-sky-300">{'{mainTopic}'}</code> becomes "The topic is AI ethics." (from Variable Node B).<br/>
           <code className="bg-slate-700 px-1 rounded text-sky-300">{'{PREVIOUS_OUTPUT}'}</code> becomes "The topic is AI ethics." (from Node B, which is the direct input to C).<br/>
          So, the LLM receives: "Discuss The topic is AI ethics. The previous direct input was: The topic is AI ethics."
          </p>
        </section>
        <hr className="border-slate-700"/>
        <section>
          <h4 className="text-lg font-semibold text-sky-400 mb-2">Conditional Node Branching</h4>
          <p>Conditional nodes direct the workflow based on the LLM's response to their prompt. The LLM response is checked against branch conditions.</p>
          <p className="font-medium mt-2">Condition Matching Rules (applied to LLM output for the Conditional node):</p>
          <ul className="list-disc list-inside space-y-1 mt-1">
            <li><strong>Exact Match (Case-Insensitive):</strong> Condition <code className="bg-slate-700 px-1 rounded text-sky-300">Approved</code> matches "Approved", "approved".</li>
            <li><strong>Contains (Case-Insensitive):</strong> Condition <code className="bg-slate-700 px-1 rounded text-sky-300">contains 'keyword'</code> matches if LLM output includes "keyword".</li>
            <li><strong>Starts With (Case-Insensitive):</strong> Condition <code className="bg-slate-700 px-1 rounded text-sky-300">starts with 'prefix'</code> matches if LLM output begins with "prefix".</li>
            <li><strong>Default Branch:</strong> Condition <code className="bg-slate-700 px-1 rounded text-sky-300">default</code> (or empty) matches if no prior conditions met. Best as the last branch.</li>
          </ul>
        </section>
         <hr className="border-slate-700"/>
        <section>
            <h4 className="text-lg font-semibold text-sky-400 mb-2">Execution Panel</h4>
            <p>The panel at the bottom shows real-time information during a workflow run:</p>
            <ul className="list-disc list-inside space-y-1 mt-1">
                <li>Currently executing node.</li>
                <li>A log of each node: start time, end time, status, output (or value set for Variable nodes), and tokens used.</li>
                <li>After the run: total duration and total tokens for the entire workflow.</li>
            </ul>
            <p>This panel can be collapsed or expanded using the button on its header.</p>
        </section>
      </div>
    </Modal>
  );
};

export default HelpModal;